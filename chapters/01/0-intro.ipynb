{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Lil' History\n",
    "\n",
    "Before building up the notation and delving into bandit algorithms, \n",
    "it's worth thinking about where they came from and the problems \n",
    "that were initially considered when formulating the notion of \n",
    "bandit algorithms.\n",
    "\n",
    "- 1933, William R. Thompson (medical trials, blindly finishing them)\n",
    "- 1950s, Frederick Mosteller and Robert Bush (animal learning)\n",
    "\n",
    "![Animal decision making](figs/animal.png)\n",
    "\n",
    "The two-armed bandit machine!\n",
    "\n",
    "![Two-Armed Bandit](figs/twoarms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Bandits?\n",
    "\n",
    "- Simplistic adaptive learning scnearios\n",
    "- Adaptive algorithms for web interfaces, recommendation algorithms, dynamic pricing\n",
    "- Lot's of fun math (convex analysis, optimization, complexity, probability theory, information theory) \n",
    "- Growing body of literature, growing research interests\n",
    "    - 2001-2005: <1000 papers\n",
    "    - 2006-2010: 2700 papers\n",
    "    - 2011-2015: 7000 papers\n",
    "    - 2016-: 5600+ papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Classic Dilemma\n",
    "\n",
    "Say you have a slot machine:\n",
    "- Pulling the left arm, you've won \\$10 2/5 times \n",
    "- Pulling the right arm, you've won $10 1/5 times\n",
    "\n",
    "Which arm should you keep pulling if you had 10 more times to pull?\n",
    "\n",
    "Exploration versus explotation!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalizing Notation\n",
    "\n",
    "We are concerned how a **learner** interfaces with an **environment**.\n",
    "\n",
    "Specifically, we will consider a sequential game played for $n$ rounds. \n",
    "This $n$ is called the horizon. \n",
    "A $t \\in [n]$ is a specific round of game.\n",
    "\n",
    "For each $t \\in [n]$, our learner will select an action $A_t$ (in a set of possible actions $\\mathcal{A}$).\n",
    "This could be L or R if our choices are to select a left or right arm on a slot machine.\n",
    "- Side note: \"multi-arm\" or \"k-armed\" bandits refer to the # of actions. For our previous dilemma, $k=2$ and we have a 2-armed bandit problem\n",
    "\n",
    "At the end of each round, the environment reveals a reward: $X_t \\in \\mathbb{R}$. In the slot example, this may be money, but could be anything (like a binary 0/1 signal to denote whether a link or ad was clicked).\n",
    "\n",
    "At time $t$, a learner has access to its **history**: $H_{t-1} = (A_1,X_1,...,A_{t-1},X_{t-1})$.\n",
    "Obviously, at most, a learner only has access to the past and can never see the future.\n",
    "\n",
    "A **policy** ($\\pi$) is a map from a history to actions. That is, a policy is the way our learner will consider the past to inform the decision in makes for the current timestep.\n",
    "\n",
    "A common (but not the only) goal is to maximize cumulative reward: $\\sum_{t=1}^{n} X_{t}$.\n",
    "\n",
    "### Environment? Environment Class?\n",
    "\n",
    "At its core, the goal of a bandit problem is to design a learning algorithm that can function \"well\" in an unknown environment. \n",
    "In the most general sense, the term \"unknown\" may not be particularly useful.\n",
    "For this reason, bandit problems are typically associated with what's called an **environment class** ($\\mathcal{E}$).\n",
    "This is a way to set a boundary around what our algorithms are expected to be able to learn.\n",
    "\n",
    "### Regret\n",
    "\n",
    "We've already highlighted one way of looking at algorithmic performance (the cumulative reward), but one of the major ones that anyone working with bandit algorithms _needs_ to be aware of is **regret**.\n",
    "\n",
    "Here, we consider regret to be be a quantity defined between our learner and another policy $\\pi$. \n",
    "It is the difference between the reward we expect our learner to get over $n$ rounds and the rewared expected from following policy $\\pi$.\n",
    "\n",
    "In fact, we can extend this notion to compare our learner against a set of different policies $\\pi \\in \\Pi$.\n",
    "Here, regret is defined as the _max_ regret attained by not following the best algorithm in the whole set. \n",
    "Another name for this set of policies is the **competitor class**.\n",
    "\n",
    "Much like the name might suggest, this is how regretful we are that we did not use another policy when playing our game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.2, made more concrete\n",
    "\n",
    "Example 1.2 in the text introduces a stochastic Bernoulli bandit problem to illustrate what regret might look like in practice.\n",
    "\n",
    "Here, we have a bandit problem with $k$ arms where each arm has an associated [Bernoulli Distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution),\n",
    "each parameterized with 1 parameter $\\mu_i, i \\in \\{1,...,k\\}$ that defines the probability of generating a reward of 1.\n",
    "\n",
    "The competitor class we will consider is the class of constant policies--AKA, the policies of only selecting one of the $k$ arms every time. \n",
    "\n",
    "This makes sense; if we knew the $mu_i$ a priori, we could simply select the arm with the highest average value and thus gain 0 regret.\n",
    "\n",
    "Unfortunately, it is not typical for us to have that oracle knowledge. \n",
    "\n",
    "Out of curiosity, let's set $k=3$ and see how these constant algorithms would \"fair\" against one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class BernoulliArm:\n",
    "    \n",
    "    '''\n",
    "    Very basic Bernoulli Arm setup\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, mu):\n",
    "        self._mu = mu\n",
    "        \n",
    "    def __call__(self):\n",
    "        return 1 if random.random() <= self._mu else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out different probabilities!\n",
    "mus = {\n",
    "    0: 0.3,\n",
    "    1: 0.35,\n",
    "    2: 0.25,\n",
    "}\n",
    "\n",
    "bandit = {k: BernoulliArm(v) for k, v in mus.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000 # set a time horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 550, 1: 731, 2: 513}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = {0: 0, 1: 0, 2: 0}\n",
    "for t in range(n):\n",
    "    for k, arm in bandit.items():\n",
    "        scores[k] += arm()\n",
    "\n",
    "# according to our expectation, arm 1 should have the highest reward!\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 600.0, 1: 700.0, 2: 500.0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# difference from \"expected\" probs. \n",
    "{k: n*mu for k, mu in mus.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 181, 1: 0, 2: 218}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regrets = {}\n",
    "for k1, s1 in scores.items():\n",
    "    max_regret = 0\n",
    "    for k2, s2 in scores.items():\n",
    "        d = s2 - s1\n",
    "        if d > max_regret:\n",
    "            max_regret = d\n",
    "    regrets[k1] = max_regret\n",
    "\n",
    "regrets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Couple More Remarks on Problem Formulations \n",
    "\n",
    "- Worst-case regret, always\n",
    "    - A \"good\" learner will achieve sub-linear (linear is never getting a positive reward) \n",
    "    - When do we achieve log(n)? sqrt(n)? \n",
    "    - How \"good\" is \"good\"?\n",
    "- The more general the environment class, the less specific statements we can make \n",
    "- Different stationary stochastic settings: Normal versus Bernoulli, combinatorial?\n",
    "- Adversarial bandits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of Bandit Algorithms \n",
    "\n",
    "- Choices today altering the available choices and rewards tomorrow (RL)\n",
    "- Rewards are not always observable (partial monitoring) \n",
    "- Strategic agents (game theory) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real World Applications\n",
    "\n",
    "- A/B Testing\n",
    "    - Adaptively selecting between new interfaces for a website (no need to end it!)\n",
    "- Ad Placement\n",
    "    - Is an ad clicked or not? \n",
    "    - User context \n",
    "- Recommendation Algorithms\n",
    "    - Online learning versus matrix factorization\n",
    "- Network or Route Planning\n",
    "    - Time of shortest path\n",
    "    - Similar paths may overlap; more intentional and efficient learning? \n",
    "- Dynamic Pricing\n",
    "    - Never observe the true value, only know true value was higher than set price\n",
    "- Tree Search\n",
    "    - UCT algorithm (AlphaGO!)\n",
    "    - Overlapping game decision paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
