{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Bandits\n",
    "\n",
    "This chapter formally introduces stochastic bandits. \n",
    "Specifically, this notebook will attempt to make plain\n",
    "the assumptions and definitions needed to understand\n",
    "stochastic bandits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Assumptions\n",
    "\n",
    "A stochastic bandit is a collection of distributions: \n",
    "\n",
    "$$\\nu = (P_a : a \\in \\mathcal{A})$$\n",
    "\n",
    "If $\\mathcal{A}$ is the set of available actions, then $a$ is any action in the set. \n",
    "As such, $P_a$ is the *probability distribution* associated with action $a$. \n",
    "Furthermore, when we select $a$ as our action, $P_a$ is the distribution we will draw our reward from.\n",
    "\n",
    "A learner will interact with a bandit problem for $n$ rounds.\n",
    "This $n$ is our time-horizon.\n",
    "In every round $t \\in \\{1,...,n\\}$, the learner is responsible for selecting an $A_t \\in \\mathcal{A}$.\n",
    "In other words, at every time step, a learner must choose an action to take (and that must be from the set of possible actions, as defined).\n",
    "\n",
    "When a learner selects $A_t \\in \\mathcal{A}$ (an action in the available choices), the environment samples a reward from the associated probability distribution:\n",
    "\n",
    "$$\n",
    "X_t \\sim P_{A_t}\n",
    "$$\n",
    "\n",
    "In plain English: We select one of the arms as our action. \n",
    "In doing so, that arm's associated probability distribution is sampled which generates the reward $X_t$ for this specific time step.\n",
    "This reward is shown to the learner (allowing for learning to take place).\n",
    "\n",
    "The selection of arms and receiving of rewards continues back-and-forth, creating a sequence of actions:\n",
    "\n",
    "$$\n",
    "A_1,X_1,A_2,X_2,...,A_n,X_n\n",
    "$$\n",
    "\n",
    "This sequence necessarily needs to obey two assumptions:\n",
    "\n",
    "- $p(X_t | A_1,X_1,...,A_{t-1},X_{t-1},A_t) = P_{A_t}$\n",
    "    - The conditional distribution over a reward for the current time step is simply equal to the probability distribution of the arm selected by action $A_t$\n",
    "    - This helps to assert that $X_t$ is drawn from the distribution associated with the arm\n",
    "- The law for how a learner selects $A_t$ given the sequence $A_1,X_1,A_2,X_2,...,A_{t-1},X_{t-1}$ is simply $\\pi_t(\\cdot | A_1,X_1,A_2,X_2,...,A_{t-1},X_{t-1})$\n",
    "    - This is saying that a learner's policy for selecting an action is solely determined by the past sequence of events\n",
    "    - In other words, our agent can't see the future! (~obvious, but important~)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Summary / Parameters of Note!\n",
    "\n",
    "- $|\\mathcal{A}|$ -- the number of arms\n",
    "- $P_a$ -- the probability distribution associated with an arm\n",
    "- $n$ -- the time horizon, the limit to the number of time steps\n",
    "- $t$ -- the current time step\n",
    "- $A_t$ -- the action selected at a specific time step\n",
    "- $X_t$ -- the reward drawn at times step $t$, given the selection of $A_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objective\n",
    "\n",
    "The goal of a bandit problem is (typically) to maximize the reward!\n",
    "\n",
    "$$\n",
    "S_n = \\sum_{t=1}^{n} X_t\n",
    "$$\n",
    "\n",
    "The textbook highlights 3 reasons why MAB problems are not *optimization* problems:\n",
    "- Uncertainty in time horizon $n$... \n",
    "    - Do we know $n$? \n",
    "    - Is it important for our algorithm?\n",
    "        - Adjustments can be made to bounds to handle uncertainty in the horizon, but at what penalty?\n",
    "- Cumulative reward is a random quantity\n",
    "    - Even if we know the exact distributions, we still need a measure of utility to understand how to interact with this specific instance\n",
    "- The reward distributions are unknown! \n",
    "    - This is the beauty of bandits; we know the arms but don't know the distributions associated with them!\n",
    "    - Must learn to estimate potential reward and maximize it\n",
    "    - Must do it efficiently as possible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge and Environment Class\n",
    "\n",
    "Typically a learner has some type of information about the type of problems/environments that it will face. \n",
    "This information is the environment class $\\mathcal{E}$.\n",
    "\n",
    "### Unstructured Bandits\n",
    "\n",
    "A class where the actions ($\\mathcal{A}$) are finite and there are a set of distributions $\\mathcal{M_a}$ for each $a \\in \\mathcal{A}$ such that:\n",
    "\n",
    "$$\n",
    "\\mathcal{E} = \\{ \\nu = (P_a:a \\in \\mathcal{A}) : P_a \\in \\mathcal{M_a}, \\forall a \\in \\mathcal{A} \\}\n",
    "$$\n",
    "\n",
    "Essentially this just means that if you pull arm a, then you won't learn anything new about arm b. \n",
    "In other words, for unstructured bandits, the learner only learns about the action it chooses. \n",
    "Nothing else is gained about the other arms.\n",
    "\n",
    "Some examples:\n",
    "- A 5-armed bandit where every arm is a Bernoulli distribution\n",
    "- A 4-armed bandit where we know all the arms are drawn from a Gaussian distribution with variance = 1\n",
    "- A A/B test with 11 site variations where click through is modelled with Bernoulli distributions\n",
    "\n",
    "The goal is to use some knowledge about the desired problem (that it can be modelled with distributions of a certain property or class), and that becomes the mechanism for formally analyzing expected performance.\n",
    "\n",
    "### Structured Bandits\n",
    "\n",
    "Structured bandits are classes of bandits where a learner can infer something **more** about the other arms, based on some prior knowledge about how arms relate.\n",
    "In short, arms that are _never_ played can still be learned about.\n",
    "\n",
    "For example (Ex. 4.1), if there are two Bernoulli arms with means $\\theta$ and $1-\\theta$, then by pulling one arm a learner can begin to infer how the other will behave. There is only one parameter to really estimate between the two arms.\n",
    "\n",
    "Another example (Ex. 4.2) gives a situation where the action space spans d-dimensional real space ($\\mathcal{A} \\subset \\mathbb{R}^d$). \n",
    "The mean of each _arm_ is estimated by the inner product between its representation in this space and some unknown vector parameter $\\theta$. \n",
    "One could perhaps consider this as finding the optimal action vector in a d-dimensional space, as measured by the inner product.\n",
    "\n",
    "A final example (Ex. 4.3) features an action space that is a path through a graph. \n",
    "Edges are removed with random probability and the goal is to find a path where no edges are removed. \n",
    "Each time a path is tried, information is gained about the edges selected \n",
    "(i.e. the probability an edge does not get removed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regret\n",
    "\n",
    "For a given bandit problem (represented by a collection of distributions, $\\nu$)\n",
    "and the policy of a learner, $\\pi$,\n",
    "the equation for regret is:\n",
    "\n",
    "$$\n",
    "R_n(\\pi, \\nu) = n \\mu^{*}(\\nu) - \\mathbb{E}\\left[ \\sum_{t=1}^n X_t \\right]\n",
    "$$\n",
    "\n",
    "This says the regret is measured as the difference between the reward gained by choosing the optimal action every step and the expected reward our learner will gain through following policy $\\pi$.\n",
    "\n",
    "A couple of things to be aware of from Lemma 4.4: \n",
    "- $R_n(\\pi, \\nu) \\geq 0$, $\\forall \\pi$\n",
    "    - This is saying that regret is never negative. A learner never does better than choosing the optimal action at every step\n",
    "- The policy $\\pi$ that chooses an $A_t \\in \\text{argmax}_a{\\mu_a}$ $\\forall t$ statisfies $R_n = 0$\n",
    "    - Again, this emphasizes that only the policy of choosing the optimal action at **every step** achieves zero regret.\n",
    "    - This indicates that a learner should expect some positive regret \n",
    "- If $R_n = 0$ then $\\mathbb{P}(\\mu_{A_t}=\\mu^{*}) = 1 \\forall t \\in [n]$\n",
    "    - Another assertion that only by having oracle knowledge of the optimal actions can a learner achieve zero regret\n",
    "    \n",
    "As one might infer, all of this is to say that some regret **will** occur. \n",
    "It then becomes a question of \"how much regret?\"\n",
    "Or, perhaps, \"how little regret can I guarantee?\"\n",
    "\n",
    "A couple of example goals from the text: \n",
    "- $\\forall \\nu \\in \\mathcal{E}: \\lim_{n \\rightarrow \\infty} \\frac{R_n}{n} = 0$\n",
    "    - Asserting that the regret will be sublinear in $n$\n",
    "    - Over time, learning will assure our learner finds the optimal action and incurs no or little regret\n",
    "- $\\forall \\nu \\in \\mathcal{E}: R_n \\leq Cn^{p}$, $C > 0$ and $p < 1$\n",
    "    - A tighter bound; sub-linear but specifically polynomial in $n$\n",
    "- $\\forall n \\in \\mathbb{N}, \\nu \\in \\mathbb{E}$, $R_n \\leq C(\\nu)f(n)$\n",
    "    - Decomposing the regret into two components:\n",
    "        - Regret due to the instance of the particular problem\n",
    "        - Regret as a function of the time horizon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposing the Regret\n",
    "\n",
    "A very important notion in bandit algorithms is that of the suboptimality gap (action gap, immediate regret) of a given action $a$:\n",
    "\n",
    "$$\n",
    "\\Delta_{a}(\\nu) = \\mu^{*}(\\nu) - \\mu_{a}(\\nu)\n",
    "$$\n",
    "\n",
    "As the name might suggest, this is the regret **immediately** incurred by not choosing the optimal action.\n",
    "In other words, this is the difference between the expected optimal reward and the reward received by selecting arm $a$.\n",
    "\n",
    "Additionally, using indicator variables, we can define a random variable to denote the number of times an action was selected:\n",
    "\n",
    "$$\n",
    "T_{a}(t) = \\sum_{s=1}^{t} \\mathbb{I}\\{A_s = a\\}\n",
    "$$\n",
    "\n",
    "The beauty of this definition is that it allows us to phrase a learner's regret as:\n",
    "\n",
    "$$\n",
    "R_n = \\sum_{a \\in \\mathcal{A}} \\Delta_a \\mathbb{E}[T_a(n)]\n",
    "$$\n",
    "\n",
    "Or, the sum of the products of the number of times an arm was selected and the regret incurred by selecting that arm (which could be zero if the arm is the optimal one!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
